{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Ao5etbwLxNED"
            },
            "source": [
                "# Profile"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "executionInfo": {
                    "elapsed": 200845,
                    "status": "ok",
                    "timestamp": 1615451946014,
                    "user": {
                        "displayName": "Ryan",
                        "photoUrl": "",
                        "userId": ""
                    },
                    "user_tz": -480
                },
                "id": "3Bf8PD-jPkYa"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
                "datadir = '../kingston'\n",
                "libdir = '.'\n",
                "outputdir = '.'\n",
                "otherdir = '.'\n",
                "\n",
                "train_bs_ = 16 # train_batch_size\n",
                "valid_bs_ = 128 # valid_batch_size\n",
                "num_workers_ = 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qu0pJE-UxU7u"
            },
            "source": [
                "# CFG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "executionInfo": {
                    "elapsed": 200843,
                    "status": "ok",
                    "timestamp": 1615451946014,
                    "user": {
                        "displayName": "Ryan",
                        "photoUrl": "",
                        "userId": ""
                    },
                    "user_tz": -480
                },
                "id": "6fBgDehlNAAl"
            },
            "outputs": [],
            "source": [
                "class CFG:\n",
                "    seed=42\n",
                "    device='GPU' # ['TPU', 'GPU']\n",
                "    nprocs=1 # [1, 8]\n",
                "    num_workers=num_workers_\n",
                "    train_bs=train_bs_\n",
                "    valid_bs=valid_bs_\n",
                "    fold_num=5 \n",
                "    \n",
                "    target_cols=[\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"OT\"]\n",
                "    num_classes=8 \n",
                "    \n",
                "    normalize_mean=[0.4824, 0.4824, 0.4824] \n",
                "    normalize_std=[0.22, 0.22, 0.22] \n",
                "    \n",
                "    fold_list=[0]\n",
                "\n",
                "    model_arch=\"efficientnet-b0\" \n",
                "    img_size=512 \n",
                "    croped_img_size = 320 # 裁剪后的图片尺寸\n",
                "    weight_path = f\"{outputdir}/efficientnet-b0_109_fold0_epoch13.pth\" "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "p4w3IC-Qvcyq"
            },
            "source": [
                "# Import"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 222046,
                    "status": "ok",
                    "timestamp": 1615451967224,
                    "user": {
                        "displayName": "Ryan",
                        "photoUrl": "",
                        "userId": ""
                    },
                    "user_tz": -480
                },
                "id": "4U1yFOTSM5Jw",
                "outputId": "5e7b5f5c-88ed-40ff-f025-4c28bff1fd43"
            },
            "outputs": [],
            "source": [
                "# !pip install -q git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
                "    \n",
                "import sys; \n",
                "\n",
                "package_paths = [f'{libdir}pytorch-image-models-master']\n",
                "for pth in package_paths:\n",
                "    sys.path.append(pth)\n",
                "    \n",
                "import ast\n",
                "from glob import glob\n",
                "import cv2\n",
                "from skimage import io\n",
                "import os\n",
                "from datetime import datetime\n",
                "import time\n",
                "import random\n",
                "from tqdm import tqdm\n",
                "from contextlib import contextmanager\n",
                "import math\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import sklearn\n",
                "from sklearn.metrics import roc_auc_score, log_loss\n",
                "from sklearn import metrics\n",
                "from sklearn.model_selection import GroupKFold, StratifiedKFold, KFold\n",
                "import torch\n",
                "import torchvision\n",
                "from torchvision import transforms\n",
                "from torch import nn\n",
                "from torch.utils.data import Dataset,DataLoader\n",
                "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
                "from torch.nn.modules.loss import _WeightedLoss\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from torch.optim import Adam, SGD, AdamW\n",
                "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
                "from warmup_scheduler import GradualWarmupScheduler\n",
                "import timm\n",
                "import warnings\n",
                "import joblib\n",
                "from scipy.ndimage.interpolation import zoom\n",
                "import nibabel as nib\n",
                "import pydicom as dicom\n",
                "import gc \n",
                "\n",
                "\n",
                "if CFG.device == 'TPU':\n",
                "    !pip install -q pytorch-ignite\n",
                "    import ignite.distributed as idist\n",
                "elif CFG.device == 'GPU':\n",
                "    from torch.cuda.amp import autocast, GradScaler\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "executionInfo": {
                    "elapsed": 225887,
                    "status": "ok",
                    "timestamp": 1615451971076,
                    "user": {
                        "displayName": "Ryan",
                        "photoUrl": "",
                        "userId": ""
                    },
                    "user_tz": -480
                },
                "id": "8X-g-a_hM_zN"
            },
            "outputs": [],
            "source": [
                "def seed_everything(seed=42):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = True \n",
                "\n",
                "seed_everything(CFG.seed)\n",
                "\n",
                "\n",
                "def get_score(y_true, y_pred):\n",
                "    scores = []\n",
                "    for i in range(y_true.shape[1]):\n",
                "        score = roc_auc_score(y_true[:,i], y_pred[:,i])\n",
                "        scores.append(score)\n",
                "    avg_score = np.mean(scores)\n",
                "    return avg_score, scores\n",
                "\n",
                "\n",
                "\n",
                "# 日志记录函数\n",
                "def init_logger(log_file=outputdir+'/train.log'):\n",
                "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
                "    logger = getLogger(__name__)\n",
                "    logger.setLevel(INFO)\n",
                "    handler1 = StreamHandler()\n",
                "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
                "    handler2 = FileHandler(filename=log_file)\n",
                "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
                "    logger.addHandler(handler1)\n",
                "    logger.addHandler(handler2)\n",
                "    return logger\n",
                "\n",
                "\n",
                "def get_timediff(time1,time2):\n",
                "    minute_,second_ = divmod(time2-time1,60)\n",
                "    return f\"{int(minute_):02d}:{int(second_):02d}\"  \n",
                "\n",
                "\n",
                "class AverageMeter(object):\n",
                "    \"\"\"Computes and stores the average and current value\"\"\"\n",
                "    def __init__(self):\n",
                "        self.reset()\n",
                "\n",
                "    def reset(self):\n",
                "        self.val = 0\n",
                "        self.avg = 0\n",
                "        self.sum = 0\n",
                "        self.count = 0\n",
                "\n",
                "    def update(self, val, n=1):\n",
                "        self.val = val\n",
                "        self.sum += val * n\n",
                "        self.count += n\n",
                "        self.avg = self.sum / self.count\n",
                "\n",
                "\n",
                "def asMinutes(s):\n",
                "    m = math.floor(s / 60)\n",
                "    s -= m * 60\n",
                "    return '%dm %ds' % (m, s)\n",
                "\n",
                "\n",
                "def timeSince(since, percent):\n",
                "    now = time.time()\n",
                "    s = now - since\n",
                "    es = s / (percent)\n",
                "    rs = es - s\n",
                "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
                "\n",
                "\n",
                "def get_img(path):\n",
                "    im_bgr = cv2.imread(path)\n",
                "    im_rgb = im_bgr[:, :, ::-1]\n",
                "    return im_rgb\n",
                "\n",
                "\n",
                "def load_dicom(path):\n",
                "    \"\"\"\n",
                "    This supports loading both regular and compressed JPEG images. \n",
                "    See the first sell with `pip install` commands for the necessary dependencies\n",
                "    \"\"\"\n",
                "    img = dicom.dcmread(path)\n",
                "    img.PhotometricInterpretation = 'YBR_FULL'\n",
                "    data = img.pixel_array\n",
                "    data = data - np.min(data)\n",
                "    if np.max(data) != 0:\n",
                "        data = data / np.max(data)\n",
                "    # data = (data * 255).astype(np.uint8)\n",
                "    return data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "g6YIAKXXe81-"
            },
            "source": [
                "# 87Sampler Peek Mask"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# study_train_df = pd.read_csv(f'{datadir}/seg_25d.csv')\n",
                "# print('train_df shape:', study_train_df.shape)\n",
                "# study_train_df.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import segmentation_models_pytorch as smp\n",
                "\n",
                "# def build_model():\n",
                "#     model = smp.Unet(\n",
                "#         encoder_name=CFG.model_arch,    # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
                "#         encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
                "#         in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
                "#         classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n",
                "#         activation=None,\n",
                "#     )\n",
                "#     model.to(device)\n",
                "#     return model\n",
                "\n",
                "# def load_model(path):\n",
                "#     model = build_model()\n",
                "#     model.load_state_dict(torch.load(path)[\"model\"])\n",
                "#     model.eval()\n",
                "#     return model\n",
                "\n",
                "# model = load_model(CFG.weight_path)\n",
                "# model.eval()\n",
                "# gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import random\n",
                "# rand_idx = random.randint(0,len(study_train_df))\n",
                "# print(f\"rand_idx:{rand_idx}\")\n",
                "# example = study_train_df.iloc[rand_idx]\n",
                "# exa_image = np.load(example[\"image_path\"])\n",
                "# exa_mask = np.load(example[\"mask_path\"])\n",
                "# print(f\"exa_image.shape: {exa_image.shape}, exa_mask.shape: {exa_mask.shape}\")\n",
                "\n",
                "# exa_image = np.expand_dims((exa_image/255).transpose(2,0,1), 0)\n",
                "# exa_image = torch.from_numpy(exa_image)\n",
                "# exa_image = exa_image.to(device, dtype=torch.float)\n",
                "# with torch.no_grad():\n",
                "#     y_pred = model(exa_image)\n",
                "# y_pred = y_pred.sigmoid() ####\n",
                "# y_pred = (y_pred).to('cpu').numpy()\n",
                "# slice_pred = y_pred[0] # 8 * img_size * img_size\n",
                "# slice_mask_argmax = slice_pred.argmax(0) # img_size * img_size\n",
                "# slice_mask_max = slice_pred.max(0) # img_size * img_size\n",
                "# slice_mask = np.where(slice_mask_max>0.5, slice_mask_argmax, 0).astype(np.uint8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from matplotlib.patches import Rectangle\n",
                "\n",
                "# plt.figure(figsize=(30, 20))\n",
                "# img = exa_image[0][1].cpu().numpy() # 512*512; 0-1;\n",
                "# slice_mask = slice_mask # 512*512; 0or1;\n",
                "# label_mask = exa_mask[:,:,1].astype(\"uint8\") # 512*512; 0-8 classes;\n",
                "# plt.subplot(1, 3, 1); plt.imshow(img); plt.axis('OFF'); # 512*512; 0-1;\n",
                "# plt.subplot(1, 3, 2); plt.imshow(slice_mask); plt.axis('OFF');\n",
                "# plt.subplot(1, 3, 3); \n",
                "# plt.imshow(exa_mask[:,:,1]); \n",
                "# plt.axis('OFF');\n",
                "# # plt.subplot(1, 3, 3); plt.imshow(slice_mask); plt.imshow(img,alpha=0.7); plt.axis('OFF');\n",
                "# # # plt.colorbar()\n",
                "# plt.tight_layout()\n",
                "# plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2019Train CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "study_train_df = pd.read_csv(f'{datadir}/train.csv')\n",
                "print('train_df shape:', study_train_df.shape)\n",
                "study_train_df.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seg_paths = glob(f\"{datadir}/segmentations/*\")\n",
                "seg_gt_list = [path.split('/')[-1][:-4] for path in seg_paths]\n",
                "print(len(seg_gt_list))\n",
                "seg_gt_list[:3]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "study_train_df = study_train_df[~study_train_df[\"StudyInstanceUID\"].isin(seg_gt_list)]\n",
                "study_train_df.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_slice_list = []\n",
                "for file_name in tqdm(study_train_df[\"StudyInstanceUID\"].values):\n",
                "    train_image_path = glob(f\"{datadir}/train_images/{file_name}/*\")\n",
                "    train_image_path = sorted(train_image_path, key=lambda x:int(x.split(\"/\")[-1].replace(\".dcm\",\"\")))\n",
                "    for path_idx in range(len(train_image_path)):\n",
                "        path1 = \"nofile\" if path_idx-1 < 0 else train_image_path[path_idx-1].replace(f\"{datadir}/\", \"\")\n",
                "        path2 = train_image_path[path_idx].replace(f\"{datadir}/\", \"\")\n",
                "        path3 = \"nofile\" if path_idx+1 >= len(train_image_path) else train_image_path[path_idx+1].replace(f\"{datadir}/\", \"\")\n",
                "        slice_num = int(path2.split(\"/\")[-1].replace(\".dcm\",\"\"))\n",
                "        train_slice_list.append([f\"{file_name}_{slice_num}\", file_name, slice_num, path1, path2, path3])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df = pd.DataFrame(train_slice_list, columns=[\"id\", \"StudyInstanceUID\", \"slice_num\", \"path1\", \"path2\", \"path3\"])\n",
                "train_df = train_df.sort_values(['StudyInstanceUID', 'slice_num'], ascending = [True, True]).reset_index(drop=True)\n",
                "train_df.to_csv(f'{datadir}/train_slice_list.csv', index=False)\n",
                "train_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "RXBaZcUpQLqw"
            },
            "source": [
                "# Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "executionInfo": {
                    "elapsed": 225885,
                    "status": "ok",
                    "timestamp": 1615451971076,
                    "user": {
                        "displayName": "Ryan",
                        "photoUrl": "",
                        "userId": ""
                    },
                    "user_tz": -480
                },
                "id": "fuxNse_1M_r0"
            },
            "outputs": [],
            "source": [
                "# 构造 dataset类\n",
                "class TrainDataset(Dataset):\n",
                "    def __init__(self, df, transform=None):\n",
                "        self.df = df\n",
                "        self.transform = transform\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "\n",
                "        im2 = load_dicom(f\"{datadir}/{row['path2']}\")   # 512*512  \n",
                "        im2h = im2.shape[0]\n",
                "        im2w = im2.shape[1]\n",
                "\n",
                "        im1 = load_dicom(f\"{datadir}/{row['path1']}\") if row['path1'] != \"nofile\" else np.zeros((im2h, im2w))  # 512*512                                                       \n",
                "        im3 = load_dicom(f\"{datadir}/{row['path3']}\") if row['path3'] != \"nofile\" else np.zeros((im2h, im2w))  # 512*512  \n",
                "\n",
                "        if im1.shape !=  (im2h, im2w):\n",
                "            im1 = cv2.resize(im1, (im2w, im2h))\n",
                "        if im3.shape !=  (im2h, im2w):\n",
                "            im3 = cv2.resize(im3, (im2w, im2h)) \n",
                "        image_list = [im1, im2, im3]\n",
                "        image = np.stack(image_list, axis=2) # 512*512*3; 0-1\n",
                "\n",
                "        # transform\n",
                "        if self.transform:\n",
                "            augmented = self.transform(image=image)\n",
                "            image = augmented['image']\n",
                "        \n",
                "        # image = image/255.0\n",
                "        image = np.transpose(image, (2, 0, 1)) # 3*img_size*img_size; 0-1\n",
                "        return torch.from_numpy(image), row['StudyInstanceUID'], row['slice_num'] "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "executionInfo": {
                    "elapsed": 225884,
                    "status": "ok",
                    "timestamp": 1615451971077,
                    "user": {
                        "displayName": "Ryan",
                        "photoUrl": "",
                        "userId": ""
                    },
                    "user_tz": -480
                },
                "id": "kGjOdUreM_jF"
            },
            "outputs": [],
            "source": [
                "# 图像AUG策略\n",
                "from albumentations import (\n",
                "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
                "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
                "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
                "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, \n",
                "    CenterCrop, Resize, RandomCrop, GaussianBlur, JpegCompression, Downscale, ElasticTransform\n",
                ")\n",
                "import albumentations\n",
                "\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "\n",
                "def get_transforms(data):\n",
                "    if data == 'train':\n",
                "        return Compose([\n",
                "            Resize(CFG.img_size, CFG.img_size, interpolation=cv2.INTER_NEAREST),\n",
                "            HorizontalFlip(p=0.5),\n",
                "            VerticalFlip(p=0.5),\n",
                "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
                "            OneOf([\n",
                "                GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n",
                "                OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n",
                "                ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
                "            ], p=0.25),\n",
                "            CoarseDropout(max_holes=8, max_height=CFG.img_size[0]//20, max_width=CFG.img_size[1]//20,\n",
                "                            min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n",
                "            ], p=1.0)\n",
                "\n",
                "    elif data == 'light_train':\n",
                "        return Compose([\n",
                "            Resize(CFG.img_size, CFG.img_size, interpolation=cv2.INTER_NEAREST),\n",
                "            HorizontalFlip(p=0.5),\n",
                "            # VerticalFlip(p=0.5),\n",
                "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
                "            OneOf([\n",
                "                GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n",
                "                # OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n",
                "                ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
                "            ], p=0.25),\n",
                "            # CoarseDropout(max_holes=8, max_height=CFG.img_size[0]//20, max_width=CFG.img_size[1]//20,\n",
                "            #              min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n",
                "            ], p=1.0)\n",
                "\n",
                "    elif data == 'valid':\n",
                "        return Compose([\n",
                "            Resize(CFG.img_size, CFG.img_size, interpolation=cv2.INTER_NEAREST),\n",
                "        ])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 875
                },
                "executionInfo": {
                    "elapsed": 229121,
                    "status": "ok",
                    "timestamp": 1615451974322,
                    "user": {
                        "displayName": "Ryan",
                        "photoUrl": "",
                        "userId": ""
                    },
                    "user_tz": -480
                },
                "id": "h_AG-OSlBRm5",
                "outputId": "10f6d8fa-36f1-4612-ef86-7050ae144390"
            },
            "outputs": [],
            "source": [
                "from pylab import rcParams\n",
                "dataset_show = TrainDataset(\n",
                "    train_df, \n",
                "    get_transforms(\"valid\") # None, get_transforms(\"train\")\n",
                "    )\n",
                "rcParams['figure.figsize'] = 30,20\n",
                "for i in range(2):\n",
                "    f, axarr = plt.subplots(1,3)\n",
                "    idx = np.random.randint(0, len(dataset_show))\n",
                "    img, file_name, n_slice= dataset_show[idx]\n",
                "    # axarr[p].imshow(img) # transform=None\n",
                "    axarr[0].imshow(img[0]); plt.axis('OFF');\n",
                "    axarr[1].imshow(img[1]); plt.axis('OFF');\n",
                "    axarr[2].imshow(img[2]); plt.axis('OFF');"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "GbWy6sEbRl8P"
            },
            "source": [
                "# Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "executionInfo": {
                    "elapsed": 229120,
                    "status": "ok",
                    "timestamp": 1615451974323,
                    "user": {
                        "displayName": "Ryan",
                        "photoUrl": "",
                        "userId": ""
                    },
                    "user_tz": -480
                },
                "id": "ty22GW9qRkDk"
            },
            "outputs": [],
            "source": [
                "import segmentation_models_pytorch as smp\n",
                "\n",
                "def build_model():\n",
                "    model = smp.Unet(\n",
                "        encoder_name=CFG.model_arch,    # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
                "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
                "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
                "        classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n",
                "        activation=None,\n",
                "    )\n",
                "    model.to(device)\n",
                "    return model\n",
                "\n",
                "def load_model(path):\n",
                "    model = build_model()\n",
                "    model.load_state_dict(torch.load(path)[\"model\"])\n",
                "    model.eval()\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MQhPYPmZe2gC"
            },
            "source": [
                "# Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# os.makedirs(f\"{outputdir}/train_voxel\", exist_ok=True)\n",
                "# os.makedirs(f\"{outputdir}/train_voxel_mask\", exist_ok=True)\n",
                "# for filename in train_df[\"StudyInstanceUID\"].values:\n",
                "#     os.makedirs(f\"{outputdir}/train_mask/{filename}\", exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "slice_class_list = []\n",
                "voxel_crop_list = []\n",
                "def crop_voxel(voxel_mask, last_f_name):\n",
                "    area_thr = 10\n",
                "    # x\n",
                "    x_list = []\n",
                "    length = voxel_mask.shape[0]\n",
                "    for i in range(length):\n",
                "        if torch.count_nonzero(voxel_mask[i]).item() >= area_thr:\n",
                "            x_list.append(i)\n",
                "            break\n",
                "    else:\n",
                "        x_list.append(0)\n",
                "\n",
                "    for i in range(length-1, -1, -1):\n",
                "        if torch.count_nonzero(voxel_mask[i]).item() >= area_thr:\n",
                "            x_list.append(i)\n",
                "            break\n",
                "    else:\n",
                "        x_list.append(length-1)\n",
                "\n",
                "    # y\n",
                "    y_list = []\n",
                "    length = voxel_mask.shape[1]\n",
                "    for i in range(length):\n",
                "        if torch.count_nonzero(voxel_mask[:, i]).item() >= area_thr:\n",
                "            y_list.append(i)\n",
                "            break\n",
                "    else:\n",
                "        y_list.append(0)\n",
                "\n",
                "    for i in range(length-1, -1, -1):\n",
                "        if torch.count_nonzero(voxel_mask[:, i]).item() >= area_thr:\n",
                "            y_list.append(i)\n",
                "            break\n",
                "    else:\n",
                "        y_list.append(length-1)\n",
                "\n",
                "    # z\n",
                "    z_list = []\n",
                "    length = voxel_mask.shape[2]\n",
                "    for i in range(length):\n",
                "        if torch.count_nonzero(voxel_mask[:, :, i]).item() >= area_thr:\n",
                "            z_list.append(i)\n",
                "            break\n",
                "    else:\n",
                "        z_list.append(0)\n",
                "\n",
                "    for i in range(length-1, -1, -1):\n",
                "        if torch.count_nonzero(voxel_mask[:, :, i]).item() >= area_thr:\n",
                "            z_list.append(i)\n",
                "            break\n",
                "    else:\n",
                "        z_list.append(length-1)\n",
                "    # croped_voxel = voxels[x_list[0]:x_list[1]+1, y_list[0]:y_list[1]+1, z_list[0]:z_list[1]+1]\n",
                "    try:\n",
                "        croped_voxel_mask = voxel_mask[x_list[0]:x_list[1]+1, y_list[0]:y_list[1]+1, z_list[0]:z_list[1]+1]\n",
                "    except:\n",
                "        print(f\"last_f_name:{last_f_name}, voxel_mask.shape:{voxel_mask.shape}, x_list:{x_list}, y_list:{y_list}, z_list:{z_list}\")\n",
                "        x_list = [0, voxel_mask.shape[0]-1]; y_list = [0, voxel_mask.shape[1]-1]; z_list = [0, voxel_mask.shape[2]-1]\n",
                "        croped_voxel_mask = voxel_mask\n",
                "    voxel_crop_list.append([last_f_name, voxel_mask.shape[1], x_list[0], x_list[1]+1, y_list[0], y_list[1]+1, z_list[0], z_list[1]+1])\n",
                "\n",
                "    # croped_voxel = croped_voxel.to('cpu').numpy() # bs*img_size*img_size; 0-8 classes\n",
                "    croped_voxel_mask = croped_voxel_mask.to('cpu').numpy().astype(np.uint8) # bs*img_size*img_size; 0-8 classes\n",
                "    for x_idx in range(croped_voxel_mask.shape[0]):\n",
                "        slice_mask = croped_voxel_mask[x_idx]\n",
                "\n",
                "        unique, counts = np.unique(slice_mask, return_counts=True)\n",
                "        if len(unique) == 1 and unique[0] == 0:\n",
                "            slice_class_list.append([last_f_name, x_idx, x_idx+x_list[0], 0])\n",
                "        elif unique[0] == 0:\n",
                "            unique = unique[1:]\n",
                "            counts = counts[1:]\n",
                "            slice_class_list.append([last_f_name, x_idx, x_idx+x_list[0]+1, unique[counts.argmax()]])\n",
                "        else:\n",
                "            slice_class_list.append([last_f_name, x_idx, x_idx+x_list[0]+1, unique[counts.argmax()]])\n",
                "        \n",
                "    return None, croped_voxel_mask"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "executionInfo": {
                    "elapsed": 229495,
                    "status": "ok",
                    "timestamp": 1615451974706,
                    "user": {
                        "displayName": "Ryan",
                        "photoUrl": "",
                        "userId": ""
                    },
                    "user_tz": -480
                },
                "id": "fchpKK8dAWQU"
            },
            "outputs": [],
            "source": [
                "test_dataset = TrainDataset(train_df, transform=get_transforms(\"valid\")) # get_transforms(\"valid\")\n",
                "test_loader = DataLoader(test_dataset, batch_size=CFG.valid_bs, shuffle=False, num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
                "\n",
                "model = load_model(CFG.weight_path)\n",
                "model.eval()\n",
                "last_f_name = \"\"\n",
                "voxel_mask = []\n",
                "# voxels = []\n",
                "for step, (images, file_names, n_slice) in tqdm(enumerate(test_loader),total=len(test_loader)):\n",
                "    images = images.to(device, dtype=torch.float) # bs*3*image_size*image_size\n",
                "    batch_size = images.size(0)\n",
                "    with torch.no_grad():\n",
                "        y_pred = model(images) # [B, 8, H, W]\n",
                "    y_pred = y_pred.sigmoid()\n",
                "    slice_mask_max = torch.max(y_pred, 1) # bs*img_size*img_size\n",
                "    slice_mask = torch.where((slice_mask_max.values)>0.5, slice_mask_max.indices+1, 0) # bs*img_size*img_size; 0-8 classes\n",
                "    slice_mask = torch.where(slice_mask==8,0,slice_mask).type(torch.uint8)\n",
                "    # slice_mask = slice_mask.to('cpu').numpy().astype(np.uint8) # bs*img_size*img_size; 0-8 classes\n",
                "    # slice_image = images[:, 1, :, :] # bs*img_size*img_size\n",
                "\n",
                "    start_idx = 0\n",
                "    for bs_idx in range(batch_size):\n",
                "        f_name = file_names[bs_idx]\n",
                "        if f_name != last_f_name:\n",
                "            voxel_mask.append(slice_mask[start_idx:bs_idx])\n",
                "            # voxels.append(slice_image[start_idx:bs_idx])\n",
                "            voxel_mask = torch.cat(voxel_mask, dim=0) # n_slice*img_size*img_size; 0-8 classes\n",
                "            # voxels = torch.cat(voxels, dim=0) # n_slice*img_size*img_size\n",
                "            if len(voxel_mask) > 0:\n",
                "                croped_voxel, croped_voxel_mask = crop_voxel(voxel_mask, last_f_name)\n",
                "            last_f_name = f_name\n",
                "            start_idx = bs_idx\n",
                "            voxel_mask = []\n",
                "            # voxels = []\n",
                "        elif bs_idx == batch_size-1:\n",
                "            voxel_mask.append(slice_mask[start_idx:batch_size])\n",
                "            # voxels.append(slice_image[start_idx:batch_size])\n",
                "voxel_mask = torch.cat(voxel_mask, dim=0)\n",
                "if len(voxel_mask) > 0:\n",
                "    croped_voxel, croped_voxel_mask = crop_voxel(voxel_mask, last_f_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### replace 87 GT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# slice_class_list = []\n",
                "# voxel_crop_list = []\n",
                "for file_name in tqdm(seg_gt_list):\n",
                "    ex_path = f\"{datadir}/segmentations/{file_name}.nii\"\n",
                "    mask = nib.load(ex_path)\n",
                "    mask = mask.get_fdata()  # convert to numpy array\n",
                "    mask = mask[:, ::-1, ::-1].transpose(1, 0, 2)\n",
                "    mask = np.clip(mask,0,8).astype(np.uint8)\n",
                "    mask = np.where(mask==8, 0, mask)\n",
                "    mask = np.ascontiguousarray(mask) # 512*512*slice\n",
                "    \n",
                "    if mask.shape[0] != 512 or mask.shape[1] != 512:\n",
                "        mask = cv2.resize(mask, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
                "    mask = mask.transpose(2,0,1)\n",
                "    assert mask.shape[1] == mask.shape[2] == 512\n",
                "    mask = torch.from_numpy(mask).to(device, dtype=torch.uint8)\n",
                "    croped_voxel, croped_voxel_mask = crop_voxel(mask, file_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# post-progress "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "datadir = '../kingston'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### voxel_crop_list & vertebra_class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "voxel_crop_df = pd.read_csv(f\"{datadir}/voxel_crop.csv\")\n",
                "# voxel_crop_df = pd.DataFrame(voxel_crop_list, columns=[\"StudyInstanceUID\", \"before_image_size\", \"x0\", \"x1\", \"y0\", \"y1\", \"z0\", \"z1\"]).sort_values(by=[\"StudyInstanceUID\"])\n",
                "# voxel_crop_df.to_csv(f\"{datadir}/voxel_crop.csv\", index=False)\n",
                "voxel_crop_df # 每个study的整体crop坐标"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "slice_class_df = pd.read_csv(f\"{datadir}/slice_class.csv\")\n",
                "# slice_class_df = pd.DataFrame(slice_class_list, columns=[\"StudyInstanceUID\", \"new_slice_num\", \"old_slice_num\", \"vertebra_class\"]).sort_values(by=[\"StudyInstanceUID\", \"new_slice_num\"])\n",
                "# slice_class_df.to_csv(f\"{datadir}/slice_class.csv\", index=False)\n",
                "slice_class_df # 每张slice的所属vertebra_class(preds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# study_id_list = []\n",
                "# slice_num_list = []\n",
                "# for file_name in tqdm(voxel_crop_df[\"StudyInstanceUID\"].values, total=len(voxel_crop_df)):\n",
                "#     train_image_path = glob(f\"{datadir}/train_images/{file_name}/*\")\n",
                "#     train_image_path = sorted(train_image_path, key=lambda x:int(x.split(\"/\")[-1].replace(\".dcm\",\"\")))\n",
                "#     slice_cnt = len(train_image_path)\n",
                "\n",
                "#     study_id_list.extend([file_name]*slice_cnt)\n",
                "#     slice_num_list.extend([int(x.split(\"/\")[-1].replace(\".dcm\",\"\")) for x in train_image_path])\n",
                "    \n",
                "# all_slice_df = pd.DataFrame({\"StudyInstanceUID\":study_id_list, \"slice_num\":slice_num_list})\n",
                "# all_slice_df.to_csv(f\"{datadir}/all_slice_df.csv\", index=False)\n",
                "all_slice_df = pd.read_csv(f\"{datadir}/all_slice_df.csv\")\n",
                "print(all_slice_df.shape)\n",
                "all_slice_df.head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## gen new_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_df = []\n",
                "for idx, study_id, _, x0, x1, _, _, _, _, in tqdm(voxel_crop_df.itertuples(), total=len(voxel_crop_df)):\n",
                "    one_study = all_slice_df[all_slice_df[\"StudyInstanceUID\"] == study_id].reset_index(drop=True)\n",
                "    new_df.append(one_study[x0:x1])\n",
                "new_df = pd.concat(new_df, axis=0).reset_index(drop=True)\n",
                "new_df # 所有包含vertebra的slice"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_df = new_df.merge(voxel_crop_df, on=\"StudyInstanceUID\", how=\"left\") # merge study_crop_df\n",
                "display(new_df) # 合并了study的crop信息\n",
                "assert len(slice_class_df) == len(new_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### slice_class_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_slice_df = pd.concat([new_df, slice_class_df[[\"new_slice_num\", \"vertebra_class\"]]], axis=1)\n",
                "new_slice_df # 合并 class"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### merge train.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tr_df = pd.read_csv(f\"{datadir}/train.csv\")\n",
                "new_slice_df1 = new_slice_df.merge(tr_df, on=\"StudyInstanceUID\", how=\"left\")\n",
                "new_slice_df1 # 合并 train.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_slice_df1.to_csv(f\"{datadir}/train_slice.csv\", index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### vertebra level"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_num = 24\n",
                "vertebrae_df_list = []\n",
                "for study_id in tqdm(np.unique(new_slice_df1[\"StudyInstanceUID\"])):\n",
                "    one_study = new_slice_df1[new_slice_df1[\"StudyInstanceUID\"] == study_id].reset_index(drop=True)\n",
                "    for cid in range(1, 8):\n",
                "        one_study_cid = one_study[one_study[\"vertebra_class\"] == cid].reset_index(drop=True)\n",
                "        if len(one_study_cid) >= sample_num:\n",
                "            sample_index = np.linspace(0, len(one_study_cid)-1, sample_num, dtype=int)\n",
                "            one_study_cid = one_study_cid.iloc[sample_index].reset_index(drop=True)\n",
                "        if len(one_study_cid) < 1:\n",
                "            continue\n",
                "        slice_num_list = one_study_cid[\"slice_num\"].values.tolist()\n",
                "        arow = one_study_cid.iloc[0]\n",
                "        vertebrae_df_list.append([f\"{study_id}_{cid}\", study_id, cid, slice_num_list, arow[\"before_image_size\"], \\\n",
                "            arow[\"x0\"], arow[\"x1\"], arow[\"y0\"], arow[\"y1\"], arow[\"z0\"], arow[\"z1\"], arow[f\"C{cid}\"]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(vertebrae_df_list)/(2019*7)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# vertebrae_df = pd.DataFrame(vertebrae_df_list, columns=[\"study_cid\", \"StudyInstanceUID\", \"cid\", \"slice_num_list\", \\\n",
                "#     \"before_image_size\", \"x0\", \"x1\", \"y0\", \"y1\", \"z0\", \"z1\", \"label\"])\n",
                "# vertebrae_df.to_pickle(f\"{datadir}/vertebrae_df.pkl\")    \n",
                "vertebrae_df = pd.read_pickle(f\"{datadir}/vertebrae_df.pkl\")    \n",
                "vertebrae_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### study level"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_num = 90\n",
                "study_df_list = []\n",
                "for study_id in tqdm(np.unique(new_slice_df1[\"StudyInstanceUID\"])):\n",
                "    one_study = new_slice_df1[new_slice_df1[\"StudyInstanceUID\"] == study_id].reset_index(drop=True)\n",
                "    if len(one_study) >= sample_num:\n",
                "        sample_index = np.linspace(0, len(one_study)-1, sample_num, dtype=int)\n",
                "        one_study = one_study.iloc[sample_index].reset_index(drop=True)\n",
                "    slice_num_list = one_study[\"slice_num\"].values.tolist()\n",
                "    arow = one_study.iloc[0]\n",
                "    study_df_list.append([study_id, slice_num_list, arow[\"before_image_size\"], arow[\"x0\"], arow[\"x1\"], arow[\"y0\"], arow[\"y1\"], arow[\"z0\"], arow[\"z1\"], arow[\"patient_overall\"]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(study_df_list)/(2019)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "study_df = pd.DataFrame(study_df_list, columns=[\"StudyInstanceUID\", \"slice_num_list\", \"before_image_size\", \"x0\", \"x1\", \"y0\", \"y1\", \"z0\", \"z1\", \"label\"])\n",
                "study_df.to_pickle(f\"{datadir}/study_df_{sample_num}.pkl\") \n",
                "# study_df = pd.read_pickle(f\"{datadir}/study_df_90.pkl\")    \n",
                "study_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# plt.figure(figsize=(30, 20))\n",
                "# img = images[0].cpu().numpy().transpose(1,2,0)\n",
                "# plt.subplot(1, 3, 1); plt.imshow(slice_mask); plt.axis('OFF');\n",
                "# plt.subplot(1, 3, 2); plt.imshow(img); plt.axis('OFF');\n",
                "# plt.subplot(1, 3, 3); plt.imshow(slice_mask); plt.imshow(img,alpha=0.7); plt.axis('OFF');\n",
                "# # # plt.colorbar()\n",
                "# plt.tight_layout()\n",
                "# plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "【V】stage1.ipynb",
            "provenance": [
                {
                    "file_id": "1_ekPHJkRCuHI-A_DNUuSPRXVbsLdlZ5k",
                    "timestamp": 1611044390026
                }
            ]
        },
        "kernelspec": {
            "display_name": "Python 3.7.11 ('py37')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.11"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": true,
            "toc_window_display": false
        },
        "vscode": {
            "interpreter": {
                "hash": "b79a61544c9a744d09395b396d14bdc3ab2980641b64ddb1c7bc6d7b892900a0"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}