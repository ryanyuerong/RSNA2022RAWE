{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao5etbwLxNED"
   },
   "source": [
    "# Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 200845,
     "status": "ok",
     "timestamp": 1615451946014,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "3Bf8PD-jPkYa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "datadir = '../kingston'\n",
    "libdir = '.'\n",
    "outputdir = '.'\n",
    "otherdir = '.'\n",
    "\n",
    "train_bs_ = 16 # train_batch_size\n",
    "valid_bs_ = 32 # valid_batch_size\n",
    "num_workers_ = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu0pJE-UxU7u"
   },
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 200843,
     "status": "ok",
     "timestamp": 1615451946014,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "6fBgDehlNAAl"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed=42\n",
    "    device='GPU'\n",
    "    nprocs=1 \n",
    "    num_workers=num_workers_\n",
    "    train_bs=train_bs_\n",
    "    valid_bs=valid_bs_\n",
    "    fold_num=5 \n",
    "    \n",
    "    target_cols=[\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"OT\"]\n",
    "    num_classes=8 \n",
    "    \n",
    "    accum_iter=1 \n",
    "    max_grad_norm=1000 \n",
    "    print_freq=100 \n",
    "    normalize_mean=[0.4824, 0.4824, 0.4824] \n",
    "    normalize_std=[0.22, 0.22, 0.22] \n",
    "    \n",
    "    suffix=\"109\" \n",
    "    fold_list=[0] \n",
    "    epochs=15\n",
    "    model_arch=\"efficientnet-b0\"\n",
    "    img_size=320 \n",
    "    optimizer=\"AdamW\" \n",
    "    scheduler=\"CosineAnnealingLR\"\n",
    "    loss_fn=\"BCEWithLogitsLoss\"\n",
    "    scheduler_warmup= \"GradualWarmupSchedulerV3\"\n",
    "\n",
    "    warmup_epo=1\n",
    "    warmup_factor = 10\n",
    "    T_max= epochs-warmup_epo-2 if scheduler_warmup==\"GradualWarmupSchedulerV2\" else \\\n",
    "           epochs-warmup_epo-1 if scheduler_warmup==\"GradualWarmupSchedulerV3\" else epochs-1\n",
    "   \n",
    "    lr=5e-3 \n",
    "    min_lr=1e-6 #\n",
    "    weight_decay=0 \n",
    "    \n",
    "    n_early_stopping=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4w3IC-Qvcyq"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222046,
     "status": "ok",
     "timestamp": 1615451967224,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "4U1yFOTSM5Jw",
    "outputId": "5e7b5f5c-88ed-40ff-f025-4c28bff1fd43"
   },
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
    "    \n",
    "import sys; \n",
    "\n",
    "package_paths = [f'{libdir}pytorch-image-models-master']\n",
    "for pth in package_paths:\n",
    "    sys.path.append(pth)\n",
    "    \n",
    "import ast\n",
    "from glob import glob\n",
    "import cv2\n",
    "from skimage import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, KFold\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "import timm\n",
    "import warnings\n",
    "import joblib\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import nibabel as nib\n",
    "import pydicom as dicom\n",
    "import gc \n",
    "\n",
    "\n",
    "if CFG.device == 'TPU':\n",
    "    !pip install -q pytorch-ignite\n",
    "    import ignite.distributed as idist\n",
    "elif CFG.device == 'GPU':\n",
    "    from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6YIAKXXe81-"
   },
   "source": [
    "# helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{datadir}/seg_25d.csv')\n",
    "print('train_df shape:', train_df.shape)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 ---> background   \n",
    "1 ---> C1   \n",
    "2 ---> C2   \n",
    "...     \n",
    "8 ---> T1 - T12  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_name = \"1.2.826.0.1.3680043.780_147\"\n",
    "image_example = np.load(f\"{datadir}/seg_25d_image/{example_name}.npy\").transpose(2,0,1)\n",
    "mask_example = np.load(f\"{datadir}/seg_25d_mask/{example_name}.npy\").transpose(2,0,1)\n",
    "plt.figure(figsize=(30, 20))\n",
    "plt.subplot(1, 3, 1); plt.imshow(image_example[1]); plt.axis('OFF');\n",
    "plt.subplot(1, 3, 2); plt.imshow(mask_example[1]); plt.axis('OFF');\n",
    "plt.subplot(1, 3, 3); plt.imshow(image_example[1]); plt.imshow(mask_example[1],alpha=0.5); plt.axis('OFF');\n",
    "# plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 225887,
     "status": "ok",
     "timestamp": 1615451971075,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "6zY-6md5uWyz"
   },
   "outputs": [],
   "source": [
    "if CFG.device == 'TPU':\n",
    "    import os\n",
    "    VERSION = \"1.7\"\n",
    "    CP_V = \"36\" if ENV == \"colab\" else \"37\"\n",
    "    wheel = f\"torch_xla-{VERSION}-cp{CP_V}-cp{CP_V}m-linux_x86_64.whl\"\n",
    "    url = f\"https://storage.googleapis.com/tpu-pytorch/wheels/{wheel}\"\n",
    "    !pip3 -q install cloud-tpu-client==0.10 $url\n",
    "    os.system('export XLA_USE_BF16=1')\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    CFG.lr = CFG.lr * CFG.nprocs\n",
    "    CFG.train_bs = CFG.train_bs // CFG.nprocs\n",
    "    device = xm.xla_device()\n",
    "elif CFG.device == \"GPU\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 225887,
     "status": "ok",
     "timestamp": 1615451971076,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "8X-g-a_hM_zN"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True \n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        score = roc_auc_score(y_true[:,i], y_pred[:,i])\n",
    "        scores.append(score)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score, scores\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n",
    "\n",
    "# 日志记录函数\n",
    "def init_logger(log_file=outputdir+'/train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger(outputdir+f'/train{CFG.suffix}.log')\n",
    "\n",
    "if CFG.device=='TPU' and CFG.nprocs==8:\n",
    "    loginfo = xm.master_print\n",
    "    cusprint = xm.master_print\n",
    "else:\n",
    "    loginfo = LOGGER.info\n",
    "    cusprint = print\n",
    "\n",
    "def get_result(result_df):\n",
    "    preds = result_df[[f'pred_{c}' for c in CFG.target_cols]].values\n",
    "    labels = result_df[CFG.target_cols].values\n",
    "    score, scores = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}  Scores: {np.round(scores, decimals=4)}')\n",
    "\n",
    "\n",
    "def get_timediff(time1,time2):\n",
    "    minute_,second_ = divmod(time2-time1,60)\n",
    "    return f\"{int(minute_):02d}:{int(second_):02d}\"  \n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb\n",
    "\n",
    "\n",
    "def load_dicom(path):\n",
    "    \"\"\"\n",
    "    This supports loading both regular and compressed JPEG images. \n",
    "    See the first sell with `pip install` commands for the necessary dependencies\n",
    "    \"\"\"\n",
    "    img = dicom.dcmread(path)\n",
    "    img.PhotometricInterpretation = 'YBR_FULL'\n",
    "    data = img.pixel_array\n",
    "    data = data - np.min(data)\n",
    "    if np.max(data) != 0:\n",
    "        data = data / np.max(data)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return cv2.cvtColor(data, cv2.COLOR_GRAY2RGB), img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXBaZcUpQLqw"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 225885,
     "status": "ok",
     "timestamp": 1615451971076,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "fuxNse_1M_r0"
   },
   "outputs": [],
   "source": [
    "# 构造 dataset类\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['id'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        image = np.load(f\"{datadir}/seg_25d_image/{file_name}.npy\") # 512 * 512 * 3\n",
    "        mask = np.load(f\"{datadir}/seg_25d_mask/{file_name}.npy\") # 512 * 512 * 3\n",
    "\n",
    "        # transform\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        image = image/255.0\n",
    "\n",
    "\n",
    "        real_mask = np.zeros([CFG.num_classes, CFG.img_size, CFG.img_size])  # 8 * img_size * img_size\n",
    "        for idx in range(CFG.num_classes):\n",
    "            mask_bool = (mask[:,:,1] == (idx+1))\n",
    "            real_mask[idx] = mask_bool \n",
    "        \n",
    "        image = np.transpose(image, (2, 0, 1)) # 3 * img_size * img_size\n",
    "        mask = np.transpose(mask, (2, 0, 1)) # 3 * img_size * img_size\n",
    "\n",
    "        return torch.from_numpy(image), torch.from_numpy(real_mask), torch.from_numpy(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 225884,
     "status": "ok",
     "timestamp": 1615451971077,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "kGjOdUreM_jF"
   },
   "outputs": [],
   "source": [
    "# 图像AUG策略\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, \n",
    "    CenterCrop, Resize, RandomCrop, GaussianBlur, JpegCompression, Downscale, ElasticTransform\n",
    ")\n",
    "import albumentations\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_transforms(data):\n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            Resize(CFG.img_size, CFG.img_size, interpolation=cv2.INTER_NEAREST),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
    "            OneOf([\n",
    "                GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n",
    "                OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n",
    "                ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
    "            ], p=0.25),\n",
    "            CoarseDropout(max_holes=8, max_height=CFG.img_size[0]//20, max_width=CFG.img_size[1]//20,\n",
    "                            min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n",
    "            ], p=1.0)\n",
    "\n",
    "    elif data == 'light_train':\n",
    "        return Compose([\n",
    "            Resize(CFG.img_size, CFG.img_size, interpolation=cv2.INTER_NEAREST),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            # VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
    "            OneOf([\n",
    "                GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n",
    "                # OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n",
    "                ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
    "            ], p=0.25),\n",
    "            # CoarseDropout(max_holes=8, max_height=CFG.img_size[0]//20, max_width=CFG.img_size[1]//20,\n",
    "            #              min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n",
    "            ], p=1.0)\n",
    "\n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.img_size, CFG.img_size, interpolation=cv2.INTER_NEAREST),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 875
    },
    "executionInfo": {
     "elapsed": 229121,
     "status": "ok",
     "timestamp": 1615451974322,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "h_AG-OSlBRm5",
    "outputId": "10f6d8fa-36f1-4612-ef86-7050ae144390"
   },
   "outputs": [],
   "source": [
    "# 图像示例\n",
    "from pylab import rcParams\n",
    "dataset_show = TrainDataset(\n",
    "    train_df, \n",
    "    get_transforms(\"light_train\") # None, get_transforms(\"train\")\n",
    "    )\n",
    "rcParams['figure.figsize'] = 30,20\n",
    "for i in range(2):\n",
    "    f, axarr = plt.subplots(1,3)\n",
    "    idx = np.random.randint(0, len(dataset_show))\n",
    "    img, mask, raw_mask = dataset_show[idx]\n",
    "    # axarr[p].imshow(img) # transform=None\n",
    "    axarr[0].imshow(img[1]); plt.axis('OFF');\n",
    "    axarr[1].imshow(raw_mask[1]/255, alpha=0.5); plt.axis('OFF');\n",
    "    axarr[2].imshow(img[1]); axarr[2].imshow(raw_mask[1]/255,alpha=0.5); plt.axis('OFF');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbWy6sEbRl8P"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229120,
     "status": "ok",
     "timestamp": 1615451974323,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "ty22GW9qRkDk"
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def build_model():\n",
    "    model = smp.Unet(\n",
    "        encoder_name=CFG.model_arch,    # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=CFG.num_classes,        # model output channels (number of classes in your dataset)\n",
    "        activation=None,\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(path):\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\n",
    "DiceLoss    = smp.losses.DiceLoss(mode='multilabel')\n",
    "BCELoss     = smp.losses.SoftBCEWithLogitsLoss()\n",
    "LovaszLoss  = smp.losses.LovaszLoss(mode='multilabel', per_image=False)\n",
    "TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n",
    "\n",
    "def dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n",
    "    y_true = y_true.to(torch.float32)\n",
    "    y_pred = (y_pred>thr).to(torch.float32)\n",
    "    inter = (y_true*y_pred).sum(dim=dim)\n",
    "    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n",
    "    dice = ((2*inter+epsilon)/(den+epsilon)).mean(dim=(1,0))\n",
    "    return dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229118,
     "status": "ok",
     "timestamp": 1615451974323,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "4-YN_f3O3haE"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    if CFG.device == 'GPU':\n",
    "        scaler = GradScaler()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    for step, (images, masks, raw_mask) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        masks = masks.to(device, dtype=torch.float)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        if CFG.device == 'GPU':\n",
    "            with autocast(enabled=True):\n",
    "                y_preds = model(images)\n",
    "                loss = criterion(y_preds, masks)\n",
    "            # record loss\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            if CFG.accum_iter > 1:\n",
    "                loss = loss / CFG.accum_iter\n",
    "            scaler.scale(loss).backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "            if (step + 1) % CFG.accum_iter == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        elif CFG.device == 'TPU':\n",
    "            y_preds = model(images)\n",
    "            loss = criterion(y_preds, masks)\n",
    "            # record loss\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            if CFG.accum_iter > 1:\n",
    "                loss = loss / CFG.accum_iter\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "            if (step + 1) % CFG.accum_iter == 0:\n",
    "                xm.optimizer_step(optimizer, barrier=True)\n",
    "                optimizer.zero_grad()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            cusprint('Epoch: [{0}][{1}/{2}] '\n",
    "                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                'Grad: {grad_norm:.4f}  '\n",
    "                'LR: {lr:.7f}  '\n",
    "                .format(\n",
    "                epoch, step, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses,\n",
    "                remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                grad_norm=grad_norm,\n",
    "                lr=optimizer.param_groups[0][\"lr\"],\n",
    "                ))\n",
    "\n",
    "    return losses.avg, optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229117,
     "status": "ok",
     "timestamp": 1615451974324,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "0_e-xfCi4PA4"
   },
   "outputs": [],
   "source": [
    "def valid_one_epoch(valid_loader, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    start = end = time.time()\n",
    "    val_scores = []\n",
    "    for step, (images, masks, raw_mask) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        masks = masks.to(device, dtype=torch.float)\n",
    "        batch_size = images.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(images)\n",
    "        loss = criterion(y_pred, masks)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # record accuracy\n",
    "        y_pred = y_pred.sigmoid() ####\n",
    "        # y_pred = y_pred.sigmoid().to('cpu').numpy()\n",
    "        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n",
    "        val_scores.append([val_dice])\n",
    "\n",
    "        if CFG.accum_iter > 1:\n",
    "            loss = loss / CFG.accum_iter\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            cusprint('EVAL: [{0}/{1}] '\n",
    "                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                .format(\n",
    "                step, len(valid_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses,\n",
    "                remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                ))\n",
    "\n",
    "    val_scores = np.mean(val_scores, axis=0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return losses.avg, val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSWR7wtDL07F"
   },
   "source": [
    "# loss & optimizer & scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229117,
     "status": "ok",
     "timestamp": 1615451974325,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "I456DtQaL0Nz"
   },
   "outputs": [],
   "source": [
    "# 自定义逐渐升温调度器\n",
    "class GradualWarmupSchedulerV3(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV3, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch >= self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQhPYPmZe2gC"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229496,
     "status": "ok",
     "timestamp": 1615451974706,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "0PbkBzMz-NZs"
   },
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_loop(df, fold):\n",
    "    loginfo(f\"========== fold: {fold} training ==========\")\n",
    "    # ====================================================\n",
    "    # loader \n",
    "    # ====================================================\n",
    "    train_folds = train_df[train_df[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid_folds = train_df[train_df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds, transform=get_transforms(data='light_train'))\n",
    "    valid_dataset = TrainDataset(valid_folds, transform=get_transforms(data='valid'))\n",
    "    if CFG.device == 'GPU':\n",
    "        train_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, shuffle=True, num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs, shuffle=False, num_workers=0, pin_memory=True, drop_last=False)\n",
    "    elif CFG.device == 'TPU':\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.train_bs, sampler=train_sampler, drop_last=True, num_workers=CFG.num_workers)\n",
    "        valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=CFG.valid_bs, sampler=valid_sampler, drop_last=False, num_workers=CFG.num_workers)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer & scheduler & loss \n",
    "    # ====================================================\n",
    "    model = build_model()\n",
    "\n",
    "    # optimizer\n",
    "    if CFG.optimizer == \"AdamW\":\n",
    "        if CFG.scheduler_warmup in [\"GradualWarmupSchedulerV2\",\"GradualWarmupSchedulerV3\"]:\n",
    "            optimizer = AdamW(model.parameters(), lr=CFG.lr/CFG.warmup_factor, weight_decay=CFG.weight_decay) \n",
    "        else:\n",
    "            optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)  \n",
    "    # scheduler\n",
    "    if CFG.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "    elif CFG.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "    elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "\n",
    "    if CFG.scheduler_warmup==\"GradualWarmupSchedulerV3\":\n",
    "        scheduler_warmup = GradualWarmupSchedulerV3(optimizer, multiplier=10, total_epoch=CFG.warmup_epo, after_scheduler=scheduler)\n",
    "\n",
    "    # loss\n",
    "    def criterion(y_pred, y_true):\n",
    "        return 0.5*BCELoss(y_pred, y_true) + 0.5*DiceLoss(y_pred, y_true)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop \n",
    "    # ====================================================\n",
    "\n",
    "    valid_acc_max=0\n",
    "    valid_acc_max_cnt=0\n",
    "    for epoch in range(CFG.epochs):\n",
    "        loginfo(f\"***** Epoch {epoch} *****\")\n",
    "        if CFG.scheduler_warmup in [\"GradualWarmupSchedulerV2\",\"GradualWarmupSchedulerV3\"]:\n",
    "            pass\n",
    "            # loginfo(f\"schwarmup_last_epoch:{scheduler_warmup.last_epoch}, schwarmup_lr:{scheduler_warmup.get_last_lr()[0]}\")\n",
    "        if CFG.scheduler=='CosineAnnealingLR':\n",
    "            loginfo(f\"scheduler_last_epoch:{scheduler.last_epoch}, scheduler_lr:{scheduler.get_last_lr()[0]}\")\n",
    "        loginfo(f\"optimizer_lr:{optimizer.param_groups[0]['lr']}\")\n",
    "                \n",
    "        start_time = time.time() # 记录当前时间\n",
    "\n",
    "        # train\n",
    "        if CFG.device == 'TPU' and CFG.nprocs == 8:\n",
    "            para_train_loader = pl.ParallelLoader(train_loader, [device])\n",
    "            avg_loss, cur_lr = train_one_epoch(para_train_loader.per_device_loader(device), model, criterion, optimizer, epoch, scheduler, device)\n",
    "        else:\n",
    "            avg_loss, cur_lr = train_one_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # valid\n",
    "        if CFG.device == 'TPU' and CFG.nprocs == 8:\n",
    "            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n",
    "            avg_val_loss, valid_scores = valid_one_epoch(para_valid_loader.per_device_loader(device), model, criterion, device)\n",
    "            preds = idist.all_gather(torch.tensor(preds)).to('cpu').numpy()\n",
    "            valid_labels = idist.all_gather(torch.tensor(valid_labels)).to('cpu').numpy()\n",
    "        else:\n",
    "            avg_val_loss, valid_scores = valid_one_epoch(valid_loader, model, criterion, device)\n",
    "\n",
    "        # scoring\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # print(\"valid_scores:\", valid_scores, type(valid_scores))\n",
    "        valid_scores = valid_scores[0]\n",
    "        loginfo(f'Epoch {epoch} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        loginfo(f'Epoch {epoch} - Score: {valid_scores:.4f}')\n",
    "        \n",
    "        if CFG.scheduler_warmup in [\"GradualWarmupSchedulerV2\",\"GradualWarmupSchedulerV3\"]:\n",
    "            scheduler_warmup.step()\n",
    "        elif CFG.scheduler == \"ReduceLROnPlateau\":\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif CFG.scheduler in [\"CosineAnnealingLR\", \"CosineAnnealingWarmRestarts\"]:\n",
    "            scheduler.step()\n",
    "\n",
    "        if CFG.device == 'GPU':\n",
    "            torch.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n",
    "        elif CFG.device == 'TPU':\n",
    "            xm.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n",
    "\n",
    "        # early stopping \n",
    "        if valid_scores > valid_acc_max:\n",
    "            valid_acc_max = valid_scores\n",
    "            valid_acc_max_cnt=0\n",
    "            best_acc_epoch = epoch\n",
    "        else:\n",
    "            valid_acc_max_cnt+=1\n",
    "\n",
    "\n",
    "        if valid_acc_max_cnt >= CFG.n_early_stopping:\n",
    "            if CFG.device == 'GPU':\n",
    "                torch.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n",
    "            elif CFG.device == 'TPU':\n",
    "                xm.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n",
    "            print(\"early_stopping\")\n",
    "            break\n",
    "        \n",
    "        if CFG.device == 'GPU':\n",
    "            torch.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')\n",
    "        elif CFG.device == 'TPU':\n",
    "            xm.save({'model': model.state_dict()}, outputdir+f'/{CFG.model_arch}_{CFG.suffix}_fold{fold}_epoch{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229495,
     "status": "ok",
     "timestamp": 1615451974706,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "fchpKK8dAWQU"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    for fold in range(CFG.fold_num):\n",
    "        if fold in CFG.fold_list:\n",
    "            train_loop(train_df, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puIBTjW3BT3b"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 547023,
     "status": "ok",
     "timestamp": 1615455708786,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "dIRATOrSAWBV",
    "outputId": "19854189-6017-42a2-acbb-e6df8bb311df"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(CFG.suffix)\n",
    "    if CFG.device == 'TPU':\n",
    "        def _mp_fn(rank, flags):\n",
    "            torch.set_default_tensor_type('torch.FloatTensor')\n",
    "            a = main()\n",
    "        FLAGS = {}\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=CFG.nprocs, start_method='fork')\n",
    "    elif CFG.device == 'GPU':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1615455708787,
     "user": {
      "displayName": "Ryan",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -480
    },
    "id": "dsDeYW43JSSt"
   },
   "outputs": [],
   "source": [
    "# save as cpu\n",
    "if CFG.device == 'TPU':\n",
    "    for fold in range(CFG.fold_num):\n",
    "        if fold in CFG.fold_list:\n",
    "            # best score\n",
    "            state = torch.load(outputdir+f'{CFG.model_arch}_{CFG.suffix}_fold{fold}.pth')\n",
    "            torch.save({'model': state['model'].to('cpu').state_dict(), 'preds': state['preds'], 'cur_best_list': state['cur_best_list']}, \n",
    "                    outputdir+f'{CFG.model_arch}_{CFG.suffix}_fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "【V】stage1.ipynb",
   "provenance": [
    {
     "file_id": "1_ekPHJkRCuHI-A_DNUuSPRXVbsLdlZ5k",
     "timestamp": 1611044390026
    }
   ]
  },
  "interpreter": {
   "hash": "a00a3dcffa326b8072c37d7ee255aec9316998b126dc85a566132e2c156ffc5d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a00a3dcffa326b8072c37d7ee255aec9316998b126dc85a566132e2c156ffc5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
